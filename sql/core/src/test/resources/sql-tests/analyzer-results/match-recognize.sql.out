-- Automatically generated by SQLQueryTestSuite
-- !query
CREATE TABLE stock_data (symbol STRING, ts BIGINT, price DECIMAL(10,2))
USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`stock_data`, false


-- !query
INSERT INTO stock_data VALUES
  ('AAPL', 1, 100.00),
  ('AAPL', 2, 90.00),
  ('AAPL', 3, 110.00),
  ('GOOG', 1, 200.00),
  ('GOOG', 2, 180.00),
  ('GOOG', 3, 220.00),
  ('MSFT', 1, 50.00),
  ('MSFT', 2, 60.00),
  ('MSFT', 3, 55.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/stock_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/stock_data], Append, `spark_catalog`.`default`.`stock_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/stock_data), [symbol, ts, price]
+- Project [col1#x AS symbol#x, cast(col2#x as bigint) AS ts#xL, cast(col3#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x, col3#x]


-- !query
SELECT * FROM stock_data
MATCH_RECOGNIZE (
  PARTITION BY symbol
  ORDER BY ts
  MEASURES
    symbol AS sym,
    LAST(A.ts) AS last_a_ts,
    LAST(ts) AS last_ts
  PATTERN (A B)
  DEFINE
    A AS ts = 1,
    B AS ts = 2
)
-- !query analysis
Project [sym#x, last_a_ts#xL, last_ts#xL]
+- MatchRecognizeMeasures [symbol#x, _match_number#xL], [symbol#x AS sym#x, last(ts#xL, false) FILTER (WHERE (_classifier#x = a)) AS last_a_ts#xL, last(ts#xL, false) AS last_ts#xL]
   +- MatchRecognize [symbol#x], [ts#xL ASC NULLS FIRST], PatternSequence(List(PatternVariable(a), PatternVariable(b))), [(ts#xL = cast(1 as bigint)) AS a#x, (ts#xL = cast(2 as bigint)) AS b#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.stock_data
         +- Relation spark_catalog.default.stock_data[symbol#x,ts#xL,price#x] parquet


-- !query
SELECT * FROM stock_data
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES
    FIRST(ts) AS start_ts,
    LAST(ts) AS end_ts
  PATTERN (A B)
  DEFINE
    A AS ts = 1,
    B AS ts = 2
)
-- !query analysis
Project [start_ts#xL, end_ts#xL]
+- MatchRecognizeMeasures [_match_number#xL], [first(ts#xL, false) AS start_ts#xL, last(ts#xL, false) AS end_ts#xL]
   +- MatchRecognize [ts#xL ASC NULLS FIRST], PatternSequence(List(PatternVariable(a), PatternVariable(b))), [(ts#xL = cast(1 as bigint)) AS a#x, (ts#xL = cast(2 as bigint)) AS b#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.stock_data
         +- Relation spark_catalog.default.stock_data[symbol#x,ts#xL,price#x] parquet


-- !query
DROP TABLE stock_data
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.stock_data


-- !query
CREATE TABLE test_shadow (ts BIGINT, price BIGINT) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_shadow`, false


-- !query
INSERT INTO test_shadow VALUES (1, 100), (2, 100), (1, 200), (2, 200)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_shadow, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_shadow], Append, `spark_catalog`.`default`.`test_shadow`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_shadow), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as bigint) AS price#xL]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM test_shadow
MATCH_RECOGNIZE (
  PARTITION BY ts + 1 AS price
  ORDER BY ts
  MEASURES
    price AS partition_val
  PATTERN (A)
  DEFINE
    A AS ts > 0
)
-- !query analysis
Project [partition_val#xL]
+- MatchRecognizeMeasures [price#xL, _match_number#xL], [price#xL AS partition_val#xL]
   +- MatchRecognize [(ts#xL + cast(1 as bigint)) AS price#xL], [ts#xL ASC NULLS FIRST], PatternVariable(a), [(ts#xL > cast(0 as bigint)) AS a#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.test_shadow
         +- Relation spark_catalog.default.test_shadow[ts#xL,price#xL] parquet


-- !query
DROP TABLE test_shadow
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_shadow


-- !query
CREATE TABLE quant_test (ts BIGINT, price BIGINT) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`quant_test`, false


-- !query
INSERT INTO quant_test VALUES (1, 100), (2, 110), (3, 120), (4, 50)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/quant_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/quant_test], Append, `spark_catalog`.`default`.`quant_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/quant_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as bigint) AS price#xL]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM quant_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES
    FIRST(ts) AS start_ts,
    LAST(ts) AS end_ts
  PATTERN (A+)
  DEFINE
    A AS price > 80
)
-- !query analysis
Project [start_ts#xL, end_ts#xL]
+- MatchRecognizeMeasures [_match_number#xL], [first(ts#xL, false) AS start_ts#xL, last(ts#xL, false) AS end_ts#xL]
   +- MatchRecognize [ts#xL ASC NULLS FIRST], QuantifiedPattern(PatternVariable(a),OneOrMore,true), [(price#xL > cast(80 as bigint)) AS a#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.quant_test
         +- Relation spark_catalog.default.quant_test[ts#xL,price#xL] parquet


-- !query
SELECT * FROM quant_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES
    FIRST(ts) AS start_ts,
    LAST(ts) AS end_ts
  PATTERN (A+ B)
  DEFINE
    A AS price > 80,
    B AS price <= 80
)
-- !query analysis
Project [start_ts#xL, end_ts#xL]
+- MatchRecognizeMeasures [_match_number#xL], [first(ts#xL, false) AS start_ts#xL, last(ts#xL, false) AS end_ts#xL]
   +- MatchRecognize [ts#xL ASC NULLS FIRST], PatternSequence(List(QuantifiedPattern(PatternVariable(a),OneOrMore,true), PatternVariable(b))), [(price#xL > cast(80 as bigint)) AS a#x, (price#xL <= cast(80 as bigint)) AS b#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.quant_test
         +- Relation spark_catalog.default.quant_test[ts#xL,price#xL] parquet


-- !query
DROP TABLE quant_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.quant_test


-- !query
CREATE TABLE group_test (ts BIGINT, price BIGINT) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`group_test`, false


-- !query
INSERT INTO group_test VALUES (1, 50), (2, 150), (3, 60), (4, 200), (5, 10)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/group_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/group_test], Append, `spark_catalog`.`default`.`group_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/group_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as bigint) AS price#xL]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM group_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES
    FIRST(ts) AS start_ts,
    LAST(ts) AS end_ts
  PATTERN ((A B)+)
  DEFINE
    A AS price < 100,
    B AS price >= 100
)
-- !query analysis
Project [start_ts#xL, end_ts#xL]
+- MatchRecognizeMeasures [_match_number#xL], [first(ts#xL, false) AS start_ts#xL, last(ts#xL, false) AS end_ts#xL]
   +- MatchRecognize [ts#xL ASC NULLS FIRST], QuantifiedPattern(PatternSequence(List(PatternVariable(a), PatternVariable(b))),OneOrMore,true), [(price#xL < cast(100 as bigint)) AS a#x, (price#xL >= cast(100 as bigint)) AS b#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.group_test
         +- Relation spark_catalog.default.group_test[ts#xL,price#xL] parquet


-- !query
SELECT * FROM group_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES
    FIRST(ts) AS start_ts,
    LAST(ts) AS end_ts
  PATTERN ((A B)+ C)
  DEFINE
    A AS price < 100,
    B AS price >= 100,
    C AS price < 50
)
-- !query analysis
Project [start_ts#xL, end_ts#xL]
+- MatchRecognizeMeasures [_match_number#xL], [first(ts#xL, false) AS start_ts#xL, last(ts#xL, false) AS end_ts#xL]
   +- MatchRecognize [ts#xL ASC NULLS FIRST], PatternSequence(List(QuantifiedPattern(PatternSequence(List(PatternVariable(a), PatternVariable(b))),OneOrMore,true), PatternVariable(c))), [(price#xL < cast(100 as bigint)) AS a#x, (price#xL >= cast(100 as bigint)) AS b#x, (price#xL < cast(50 as bigint)) AS c#x], _match_number#x: bigint, _classifier#x: string
      +- SubqueryAlias spark_catalog.default.group_test
         +- Relation spark_catalog.default.group_test[ts#xL,price#xL] parquet


-- !query
DROP TABLE group_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.group_test


-- !query
CREATE TABLE error_test (ts BIGINT, price DECIMAL(10,2)) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`error_test`, false


-- !query
INSERT INTO error_test VALUES (1, 100.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/error_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/error_test], Append, `spark_catalog`.`default`.`error_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/error_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM error_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES LAST(ts) AS end_ts
  PATTERN (A | B)
  DEFINE
    A AS price > 50,
    B AS price > 100
)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_FEATURE.MATCH_RECOGNIZE_ALTERNATION",
  "sqlState" : "0A000",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 157,
    "fragment" : "MATCH_RECOGNIZE (\n  ORDER BY ts\n  MEASURES LAST(ts) AS end_ts\n  PATTERN (A | B)\n  DEFINE\n    A AS price > 50,\n    B AS price > 100\n)"
  } ]
}


-- !query
DROP TABLE error_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.error_test


-- !query
CREATE TABLE error_test (ts BIGINT, price DECIMAL(10,2)) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`error_test`, false


-- !query
INSERT INTO error_test VALUES (1, 100.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/error_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/error_test], Append, `spark_catalog`.`default`.`error_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/error_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM error_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES LAST(ts) AS end_ts
  PATTERN (A B)
  DEFINE
    A AS price > 50,
    A AS price > 100
)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "DUPLICATE_PATTERN_VARIABLE",
  "sqlState" : "42710",
  "messageParameters" : {
    "variableName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 155,
    "fragment" : "MATCH_RECOGNIZE (\n  ORDER BY ts\n  MEASURES LAST(ts) AS end_ts\n  PATTERN (A B)\n  DEFINE\n    A AS price > 50,\n    A AS price > 100\n)"
  } ]
}


-- !query
DROP TABLE error_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.error_test


-- !query
CREATE TABLE error_test (ts BIGINT, price DECIMAL(10,2)) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`error_test`, false


-- !query
INSERT INTO error_test VALUES (1, 100.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/error_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/error_test], Append, `spark_catalog`.`default`.`error_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/error_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM error_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES LAST(ts) AS end_ts
  PATTERN (A B)
  DEFINE
    A AS price > 50
)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNDEFINED_PATTERN_VARIABLE",
  "sqlState" : "42703",
  "messageParameters" : {
    "variableName" : "`b`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 133,
    "fragment" : "MATCH_RECOGNIZE (\n  ORDER BY ts\n  MEASURES LAST(ts) AS end_ts\n  PATTERN (A B)\n  DEFINE\n    A AS price > 50\n)"
  } ]
}


-- !query
DROP TABLE error_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.error_test


-- !query
CREATE TABLE error_test (ts BIGINT, price DECIMAL(10,2)) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`error_test`, false


-- !query
INSERT INTO error_test VALUES (1, 100.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/error_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/error_test], Append, `spark_catalog`.`default`.`error_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/error_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM error_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES LAST(A.price - B.price) AS price_diff
  PATTERN (A B)
  DEFINE
    A AS price > 50,
    B AS price > 100
)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MATCH_RECOGNIZE_AGGREGATE_MIXED_QUALIFIERS",
  "sqlState" : "42803",
  "messageParameters" : {
    "expression" : "last((price - price))",
    "qualifiers" : "a, b"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 174,
    "fragment" : "MATCH_RECOGNIZE (\n  ORDER BY ts\n  MEASURES LAST(A.price - B.price) AS price_diff\n  PATTERN (A B)\n  DEFINE\n    A AS price > 50,\n    B AS price > 100\n)"
  } ]
}


-- !query
DROP TABLE error_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.error_test


-- !query
CREATE TABLE error_test (ts BIGINT, price DECIMAL(10,2)) USING parquet
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`error_test`, false


-- !query
INSERT INTO error_test VALUES (1, 100.00)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/error_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/error_test], Append, `spark_catalog`.`default`.`error_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/error_test), [ts, price]
+- Project [cast(col1#x as bigint) AS ts#xL, cast(col2#x as decimal(10,2)) AS price#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT * FROM error_test
MATCH_RECOGNIZE (
  ORDER BY ts
  MEASURES LAST(ts) AS end_ts
  PATTERN (A B)
  DEFINE
    A AS price > RAND(),
    B AS price > 100
)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MATCH_RECOGNIZE_NONDETERMINISTIC_DEFINE",
  "sqlState" : "42845",
  "messageParameters" : {
    "expression" : "(price > rand())",
    "variableName" : "`a`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 26,
    "stopIndex" : 159,
    "fragment" : "MATCH_RECOGNIZE (\n  ORDER BY ts\n  MEASURES LAST(ts) AS end_ts\n  PATTERN (A B)\n  DEFINE\n    A AS price > RAND(),\n    B AS price > 100\n)"
  } ]
}


-- !query
DROP TABLE error_test
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.error_test
